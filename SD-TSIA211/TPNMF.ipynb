{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Lab: nonnegative matrix factorization\n",
    "\n",
    "This report of SD-TSIA211 was made by <font color=\"red\"><b>Gaspard Robert</b></font> and <font color=\"red\"><b>Th√©o Rouvet</b></font>.\n",
    "\n",
    "The beginning consists in appropriate library importations, as well as useful processing functions definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize\n",
    "import scipy.sparse\n",
    "import scipy.sparse.linalg\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_matrix_from_faces(folder='orl_faces', minidata=False):\n",
    "    # load images\n",
    "    # 400 images of size (112, 92)\n",
    "    M = []\n",
    "    if minidata is True:\n",
    "        nb_subjects = 1\n",
    "    else:\n",
    "        nb_subjects = 40\n",
    "    for subject in range(1, nb_subjects + 1\n",
    "                        ):\n",
    "        for image in range(1, 11):\n",
    "            face = plt.imread(folder + '/s' + str(subject)\n",
    "                              + '/' + str(image) + '.pgm')\n",
    "            M.append(face.ravel())\n",
    "\n",
    "    return np.array(M, dtype=float)\n",
    "\n",
    "def vectorize(W, H):\n",
    "    return np.concatenate((W.ravel(), H.ravel()))\n",
    "\n",
    "def unvectorize_M(W_H, M):\n",
    "    # number of elements in W_H is (n+p)*k where M is of size n x m\n",
    "    # W has the nk first elements\n",
    "    # H has the kp last elements\n",
    "    n, p = M.shape\n",
    "    k = W_H.shape[0] // (n + p)\n",
    "    W = W_H[:n * k].reshape((n, k))\n",
    "    H = W_H[n * k:].reshape((k, p))\n",
    "    return W, H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Download the database at https://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html.\n",
    "Uncompress the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small data to test the algorithm\n",
    "M = build_matrix_from_faces(folder=\"./orl_faces\", minidata=False)\n",
    "def unvectorize(W_H): return unvectorize_M(W_H, M)\n",
    "k = 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To see the first face:\n",
    "plt.imshow(M[0].reshape((112, 92)), cmap='gray'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 10304)\n"
     ]
    }
   ],
   "source": [
    "# Full data\n",
    "M = build_matrix_from_faces(folder='orl_faces', minidata=False)\n",
    "def unvectorize(W_H): return unvectorize_M(W_H, M)\n",
    "k = 38\n",
    "n = M.shape[0]\n",
    "p = M.shape[1]\n",
    "print(M.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The database is composed of **400 images of size $(112,92)$**, as 112*92 = 10304."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Presentation of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note $f(x,y) = (\\alpha -xy)^2$ where $\\alpha$ is a given nonnegative real number. $f$ is of $C^2$-class (i.e. it is twice continuously differentiable) on $(\\mathbb{R^+})^2$ as a polynomial function of $x$ and $y$. \n",
    "\n",
    "The Hessian matrix of $f$ is: $$\\forall x,y \\in \\mathbb{R^+}, \\nabla^2f(x,y) = \\left( {\\begin{array}{cc}\n",
    "     2y^2 & -2\\alpha + 4xy  \\\\\n",
    "     -2\\alpha + 4xy & 2x^2 \\\\\n",
    "     \\end{array} } \\right)$$\n",
    "\n",
    "Therefore, its determinant is: $$\\forall x,y \\in \\mathbb{R^+}, \\text{D}(x,y) = \\text{det}(\\nabla^2f(x,y)) = -12x^2y^2 + 16\\alpha xy -4\\alpha^2 $$\n",
    "\n",
    "We know from MDI210 that **a function twice continously differentiable is convex if and only if its Hessian Matrix is nonnegative at each point of the set**. \n",
    "\n",
    "The determinant of the Hessian matrix being the product of the eigen values of $\\nabla^2f$, in order to **prove that $f$ is not convex, it is sufficient to show that $\\exists (x_0,y_0) \\in (\\mathbb{R^+})^2, \\text{D}(x_0,y_0) < 0$**.\n",
    "\n",
    "Let us set a given $y_0 \\in \\mathbb{R^+}$. It is obvious that $\\underset{x \\rightarrow +\\infty}{\\lim} D(x,y_0) = -\\infty$, hence $\\exists x_0 \\in \\mathbb{R^+}, \\text{D}(x_0,y_0) < 0$, so the determinant of the Hessian Matrix is negative at this point, so **the Hessian matrix is not nonnegative at this point** (its eigen values are of opposite sign, so one of them is negative).\n",
    "\n",
    "To conclude, **we have shown that $f$ is not a convex function**. This result can be generalised for higher dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2\n",
    "\n",
    "For the following, we will denote $$f :  (W,H) \\mapsto \\frac{1}{2np} \\sum_{i=1}^n \\sum_{l=1}^n \\left(M_{i,l} - \\sum_{j=1}^k W_{i,j}H_{j,l}\\right)^2 = \\frac{1}{2np} \\|M-WH\\|_F^2$$\n",
    "\n",
    "as being **the objective function**, defined for couples of nonnegatives matrices. Using the canonical rules for norm and scalar product derivations, we obtain the derivatives of order 1 for $f$ : $$\\nabla f(W,H) = \\left(\\frac{\\partial f}{\\partial W}(W,H), \\text{ } \\frac{\\partial f}{\\partial H}(W,H)\\right) = \\left(\\frac{1}{np}(WH-M)H^T, \\text{ } \\frac{1}{np}W^T(WH-M) \\right) $$\n",
    "\n",
    "We then have computed the components of the gradient vector of the objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Find $W$ when $H_0$ is fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "W0, S, H0 = scipy.sparse.linalg.svds(M, k)\n",
    "W0 = np.maximum(0, W0*np.sqrt(S))\n",
    "H0 = np.maximum(0, (H0.T*np.sqrt(S)).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The singular value decompositions (svds) decompose the matrix $M$ as the product of three matrices: $M = W_0SH_0^T$, with $S$ a diagonal matrix containing the singular values of $M$ (which are **nonnegative**). \n",
    "\n",
    "We wish to have a decomposition of $M$ as the product of two matrices, so it is natural to \"split\" the matrix S using the square root: $ S = \\sqrt{S} \\cdot \\sqrt{S} $ where \n",
    "\n",
    "$$ \\sqrt{S} = \\left( {\\begin{array}{cccccccc}\n",
    "     \\sqrt{\\lambda_1} & 0 & . & . & . & . & . & 0 \\\\\n",
    "     0 & \\sqrt{\\lambda_2} & . & . & . & . & . & . \\\\\n",
    "     . & . & . & . & . & . & . & . \\\\\n",
    "     . & . & . & . & . & . & . & . \\\\\n",
    "     . & . & . & . & \\sqrt{\\lambda_r} & . & . & .  \\\\\n",
    "     . & . & . & . & . & 0 & . & .\\\\\n",
    "     . & . & . & . & . & . & . & . \\\\\n",
    "     0 & . & . & . & . & . & . & 0 \\\\\n",
    "     \\end{array} } \\right) = diag(\\sqrt{\\lambda_1}, ..., \\sqrt{\\lambda_r}, 0, ..., 0)$$ as all the $\\sqrt{\\lambda_i}$ are nonnegative.\n",
    "     \n",
    "     \n",
    "Having done so, we write $M = A \\cdot B$, with:\n",
    "* $ A = max(0, W_0 \\sqrt{S})$ as we wish to keep working with nonnegative matrices \n",
    "* $B =max(0, \\sqrt{S}H_0^T)$ for the same reason.\n",
    "\n",
    "The **advantage of this method** is that we might get the proper $M=WH_0$ directly if the computed matrices are nonnegative (and it is rather natural to initialise the algorithm with this). We can imagine starting with random matrices as well, which seems less appropriate at first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have $g(W)=\\frac{1}{2np}\\|M-WH_0\\|_F^2$\n",
    "\n",
    "Let's comupte the **gradient** of $g(W)$: $\\nabla{g}(W) = -\\frac{1}{np}(M-WH_0)H_0^T$\n",
    "\n",
    "Then, the **Hessian matrix** of g is : $\\nabla^2 g(W) = \\frac{1}{np}H_0H_0^T$\n",
    "\n",
    "It is positive semi-definite because $\\nabla^2 g(W) = N^TN$ with $N=\\frac{1}{\\sqrt{np}}H_0^T$\n",
    "Hence, we deduce that <font color=\"red\"><b> g is convex </b></font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.3\n",
    "\n",
    "We write the suggested functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(W):\n",
    "    return (1/(2*n*p))*np.linalg.norm(M-np.dot(W,H0), 'fro')**2\n",
    "\n",
    "def gradg(W):\n",
    "    return (-1/(n*p))*np.dot((M-np.dot(W,H0)),H0.T)\n",
    "\n",
    "\n",
    "# reshaped functions for scipy.optimize.check_grad\n",
    "\n",
    "def g_bis(W_bis):\n",
    "    return 1/(2*n*p)* np.linalg.norm(M.reshape(-1)-(W_bis.reshape((n,k))@H0).reshape(-1))**2\n",
    "\n",
    "def grad_g_bis(W_bis):\n",
    "    return ((M -(W_bis.reshape((n,k)))@H0)@H0.T).reshape(-1)/(-n*p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check whether the functions are correct using $W_0$ defined with the svds method, and the reshaped version of our fonctions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002216352437803484"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.optimize.check_grad(g_bis, grad_g_bis, W0.reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision yielded out is of order of magnitude $10^{-3}$ so it is rightful to assume that the functions are correct as so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.4\n",
    "\n",
    "We remind that, for all $\\gamma>0$, the proximal operator is defined as:\n",
    "\n",
    "$$\\text{prox}_{\\gamma l_{\\mathbb{R^+}}}(y) = \\text{arg}\\underset{x \\in \\mathbb{R}}{\\text{ min }} \\left( \\gamma l_{\\mathbb{R^+}}(x) + \\frac{1}{2} \\|x-y\\|^2 \\right) = \\text{arg}\\underset{x \\in \\mathbb{R^+}}{\\text{ min }} \\left( \\gamma l_{\\mathbb{R^+}}(x) + \\frac{1}{2} \\|x-y\\|^2 \\right) =\\text{arg}\\underset{x \\in \\mathbb{R^+}}{\\text{ min }} \\frac{1}{2} \\|x-y\\|^2$$ \n",
    "\n",
    "where $l_{\\mathbb{R^+}}$ is defined in the lab sheet as being **null on $\\mathbb{R^+}$ and being $+\\infty$ on $\\mathbb{R_-^*}$, hence it is equivalent to consider the $\\text{arg min}$ onto $\\mathbb{R}$ or onto $\\mathbb{R^+}$**, hence the two last equalities.\n",
    "\n",
    "\n",
    "Hence, we can say that: $$ \\begin{align} \n",
    "                           \\forall y \\in \\mathbb{R^+}, \\text{prox}_{\\gamma l_{\\mathbb{R^+}}}(y) &= y \\text{ (*) }\\\\ \n",
    "                           \\forall y < 0, \\text{prox}_{\\gamma l_{\\mathbb{R^+}}}(y) &= 0 \\text{ (**) }\n",
    "                           \\end{align}$$\n",
    "          \n",
    "          \n",
    "<u>Justification</u>:\n",
    "\n",
    "\n",
    "* $\\text{ (*) }$: We consider $y \\in \\mathbb{R^+}$. We have that $ \\forall x \\in \\mathbb{R^+}, \\frac{1}{2} \\|x-y\\|^2 \\geqslant 0$. Here, $x$ can be equal to $y$ (since it belongs to $\\mathbb{R^+}$), so **the whole function is null at this point and this point only**, hence **minimal for $x=y$**. **So in this case, $\\text{prox}_{\\gamma l_{\\mathbb{R^+}}}(y) = y$**.\n",
    "\n",
    "\n",
    "* $\\text{ (**) }$: $x$ cannot be equal to $y$ in that case. This case is equivalent to **minimising the distance between $y$ and any point of $\\mathbb{R^+}$**, i.e. **minimising $\\|x-y\\|^2$ for all $x \\in \\mathbb{R^+}$**. **Since $y<0$, the closest point of $\\mathbb{R^+}$ is $0$, hence $\\text{prox}_{\\gamma l_{\\mathbb{R^+}}}(y) = 0$**.\n",
    "\n",
    "\n",
    "\n",
    "**Therefore, we can conclude that for all $\\gamma>0$, $\\text{prox}_{\\gamma l_{\\mathbb{R^+}}}$ is actually the projection onto $ \\mathbb{R^+} $** (as it is equal to the identity function on $\\mathbb{R^+}$ and the null function on $\\mathbb{R^*_-}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.5\n",
    "\n",
    "We use this result to code a projected gradient descent method with constant step $\\gamma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projected_gradient_method(val_g, grad_g, W0, gamma, N):\n",
    "    \n",
    "    list_g = [val_g(W0)]\n",
    "    \n",
    "    for i in range(N):\n",
    "        \n",
    "        # gradient descent\n",
    "        W0 = W0 - gamma * grad_g(W0)\n",
    "        \n",
    "        # projection onto R+\n",
    "        W0 = W0 * (W0>=0) \n",
    "        \n",
    "        list_g.append(val_g(W0))\n",
    "        \n",
    "    return W0, list_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma =  3.455542973172828e-06\n",
      "The value of g(W0) after 100 iterations is: 867.6202599181838\n"
     ]
    }
   ],
   "source": [
    "L0 = np.linalg.norm(np.dot(np.transpose(H0),H0), 'fro') # Lispchitz constant\n",
    "gamma = 1/(L0)\n",
    "print(\"gamma = \", gamma)\n",
    "\n",
    "W0_final, list_g_finale = projected_gradient_method(g, gradg, W0, gamma, 100)\n",
    "print(\"The value of g(W0) after 100 iterations is:\", list_g_finale[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of $g(W_0)$ obtained means that **the distance between $M$ and $W_0 \\cdot H_0$ is still quite important after 100 iterations** of this algorithm of proximal gradient descent with constant step. This is mainly due to the value of gamma, which is very small (the order of magnitude is $10^{-6}$), so **the convergence of this algorithm is very slow** (the steps towards a local minimum are too small)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - Algorithmic refinement for the problem with $H_0$ fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.1\n",
    "\n",
    "We modify the previous algorithm, **adding a line search to find the appropriate step size at each iteration**. We choose to add a Taylor-based line search, as defined in the lecture notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = L0\n",
    "a = 0.5\n",
    "\n",
    "def taylor_line_search(val_g, grad_g, b, W0):\n",
    "    l = 0\n",
    "    grad = grad_g(W0)\n",
    "    while val_g(W0-(b*a**l)*grad) > val_g(W0)+np.trace(np.dot(np.transpose(grad),-(b*a**l)*grad))+1/(2*b*a**l)*np.linalg.norm((b*a**l)*grad, 'fro')**2:\n",
    "        l+=1\n",
    "    return (b*a**l)\n",
    "\n",
    "def projected_gradient_method_refined(val_g, grad_g, W0, N):\n",
    "    list_g = [val_g(W0)]\n",
    "    for k in range(N):\n",
    "        gamma_k = taylor_line_search(val_g, grad_g, b, W0)\n",
    "        W0 = W0 - gamma_k * grad_g(W0)\n",
    "        W0 = W0 * (W0>=0)\n",
    "        list_g.append(val_g(W0))\n",
    "    return W0, list_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of g(W0) after 100 iterations is: 483.8995981489996\n"
     ]
    }
   ],
   "source": [
    "W0_final, list_g_finale = projected_gradient_method_refined(g, gradg, W0, 100)\n",
    "print(\"The value of g(W0) after 100 iterations is:\", list_g_finale[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the **same number of iterations, this method has sharpy enhanced the convergence speed**, as we obtain almost half the value of the previous distance. This is coherent with the fact that we adapt the step size at each iteration, allowing us to go further that with a constant step of $\\gamma = \\frac{1}{L_0}$. However, it is **far longer to run** than the previous method due to the fact that we need to calculate the $\\gamma_k$ at each iteration with the line search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 - Resolution of the full problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.1\n",
    "\n",
    "We start by redefining the objective function $f$ and its gradient, as both parameters $W$ and $H$ now may vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_problem(W, H):\n",
    "    return (1/(2*n*p))*np.linalg.norm(M-np.dot(W,H), 'fro')**2\n",
    "\n",
    "def gradg_problem(W, H):\n",
    "    return ((-1/(n*p))*np.dot((M-np.dot(W,H)),H.T),(-1/(n*p))*np.dot(W.T,(M-np.dot(W,H))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we adapt the projected gradient method with line search for the variation of both $W$ and $H$. Especially, **we do not specify any given Lispchitz constant $L_0$ but we adapt it at each iteration for the line search**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def taylor_line_search_problem(val_g, grad_g, b, W0, H0):\n",
    "    l = 0\n",
    "    grad = grad_g(W0, H0)\n",
    "    while val_g(W0-(b*a**l)*grad[0], H0-(b*a**l)*grad[1]) > val_g(W0, H0)+np.trace(np.dot(grad[0].T,-(b*a**l)*grad[0]))+np.trace(np.dot(grad[1].T,-(b*a**l)*grad[1]))+1/(2*b*a**l)*np.linalg.norm((b*a**l)*grad[0], 'fro')**2+1/(2*b*a**l)*np.linalg.norm((b*a**l)*grad[1], 'fro')**2:\n",
    "        l+=1\n",
    "    return (b*a**l)\n",
    "\n",
    "def projected_gradient_method_problem(val_g, grad_g, W0, H0, N):\n",
    "    list_g = [val_g(W0,H0)]\n",
    "    for k in range(N):\n",
    "        b = np.linalg.norm(np.dot(np.transpose(H0),H0), 'fro')\n",
    "        gamma_k = taylor_line_search_problem(val_g, grad_g, b, W0, H0)\n",
    "        grad = grad_g(W0, H0)\n",
    "        W0 = W0 - gamma_k * grad[0]\n",
    "        W0 = W0 * (W0>=0)\n",
    "        H0 = H0 - gamma_k * grad[1]\n",
    "        H0 = H0 * (H0>=0)\n",
    "        list_g.append(val_g(W0,H0))\n",
    "    return W0, H0, list_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then test the method we have just implemented to solve the initial problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of g(W,H) after 100 iterations is: 224.3783388155127\n",
      " \n",
      "This method took 507.4357554912567 seconds to run\n"
     ]
    }
   ],
   "source": [
    "t1 = time()\n",
    "W, H, list_g_problem = projected_gradient_method_problem(g_problem, gradg_problem, W0, H0, 100)\n",
    "t2 = time()\n",
    "print(\"The value of g(W,H) after 100 iterations is:\", list_g_problem[-1])\n",
    "print(\" \")\n",
    "print(\"This method took\", t2 - t1, \"seconds to run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not go further than 100 iterations as this already takes more than 8 minutes to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.2\n",
    "\n",
    "By definition of $H_t$, we have $\\|M-W_tH_{t-1}\\|^2\\leqslant \\|M-W_{t-1}H_{t-1}\\|^2$.\n",
    "\n",
    "By definition of $W_t$, we have $\\|M-W_tH_t\\|^2\\leqslant \\|M-W_tH_{t-1}\\|^2$.\n",
    "\n",
    "Hence, $\\|M-W_tH_t\\|^2\\leqslant \\|M-W_{t-1}H_{t-1}\\|^2$ by transitivity of the inequalities, so **the objective function decreases at each iteration** of the algorithm of alternate minimizations. \n",
    "\n",
    "The sequence $\\left(\\|M-W_tH_t\\|^2\\right)_{t \\in \\mathbb{N}}$ is **positive and decreasing, hence it converges (and so does the algorithm)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternate minimizations method\n",
    "\n",
    "We implement the alternate minimizations method with a constant chosen step $\\gamma$ set by the user (method used to find the $\\text{arg min}$).\n",
    "\n",
    "We start by implementing the appropriate elements: the objective function for $H_0$ fixed, for $W_0$ fixed, their gradients, the projected gradient method for $H_0$ constant and the projected gradient method for $W_0$ constant, respectively.\n",
    "\n",
    "We will use these functions so as to determine the new values of $W_t$ and $H_t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_W(W, H0):\n",
    "    return g_problem(W, H0)\n",
    "\n",
    "def g_H(W0, H):\n",
    "    return g_problem(W0, H)\n",
    "\n",
    "def gradg_W(W, H0):\n",
    "    return (-1/(n*p))*np.dot((M-np.dot(W,H0)),H0.T)\n",
    "\n",
    "def gradg_H(W0, H):\n",
    "    return (-1/(n*p))*np.dot(W0.T,(M-np.dot(W0,H)))\n",
    "\n",
    "def projected_gradient_method_W(val_g, grad_g, W0, H0, gamma, N):\n",
    "    list_g = [val_g(W0, H0)]\n",
    "    for k in range(N):\n",
    "        W0 = W0 - gamma * grad_g(W0, H0)\n",
    "        W0 = W0 * (W0>=0)\n",
    "        list_g.append(val_g(W0, H0))\n",
    "    return W0, list_g\n",
    "\n",
    "def projected_gradient_method_H(val_g, grad_g, W0, H0, gamma, N):\n",
    "    list_g = [val_g(W0, H0)]\n",
    "    for k in range(N):\n",
    "        H0 = H0 - gamma * grad_g(W0, H0)\n",
    "        H0 = H0 * (H0>=0)\n",
    "        list_g.append(val_g(W0, H0))\n",
    "    return H0, list_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use all these functions to implement the algorithm as suggested in the lab sheet, and try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 1000 iterations of the alternate minimizations method with a constant step of gamma = 1\n",
      "we get for the objective function : 274.26563346822013\n",
      " \n",
      "This method took 146.73836207389832 seconds to run\n"
     ]
    }
   ],
   "source": [
    "def alternate_minimizations_constant_step(val_g_W, grad_g_W, val_g_H, grad_g_H, W0, H0, gamma, N):\n",
    "    for t in range(N):\n",
    "        W0 = projected_gradient_method_W(val_g_W, grad_g_W, W0, H0, gamma, 1)[0]\n",
    "        H0 = projected_gradient_method_H(val_g_H, grad_g_H, W0, H0, gamma, 1)[0]\n",
    "    return (W0, H0)\n",
    "\n",
    "# Trying out the algorithm for 1000 iterations\n",
    "\n",
    "gamma = 1 # step size chosen by the user\n",
    "t3 = time()\n",
    "W, H = alternate_minimizations_constant_step(g_W, gradg_W, g_H, gradg_H, W0, H0, gamma, 1000)\n",
    "t4 = time()\n",
    "\n",
    "print(\"For 1000 iterations of the alternate minimizations method with a constant step of gamma =\", gamma)\n",
    "print(\"we get for the objective function :\",  g_problem(W, H))\n",
    "print(\" \")\n",
    "print(\"This method took\", t4 - t3, \"seconds to run\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.4\n",
    "\n",
    "<u>Results in terms of running time as a function of the number of iterations</u>:\n",
    "* The projected gradient algorithm took **507 seconds (about 8 and a half minutes) to run 100 iterations**.\n",
    "* The algorithm of alternate minimizations method with constant step was **the fastest, as it took 146 seconds (about 2 and a half minutes) to run 1000 iterations**.\n",
    "\n",
    "<u>Results in terms of results for the objective function</u>:\n",
    "* **The projected gradient algorithm gave the best result**, with an **objective function of value $224$ after 100 iterations**.\n",
    "* The algorithm of alternate minimizations method with constant step gave **a result of $274$ after 1000 iterations**.\n",
    "\n",
    "Therefore, considering both algorithms, **the best compromise seems to be the use of the alternate minimizations method**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.5\n",
    "\n",
    "<u> We could choose **another stopping criterion** such as for example </u>:\n",
    "\n",
    "* the fact that $f(W,H) = \\frac{1}{2np} \\|M-WH\\|_F^2 < \\epsilon$ with $\\epsilon$ defined by the user, which may never stop if the $\\epsilon$ chosen is too small\n",
    "\n",
    "\n",
    "* the fact that between two iterations of any of the algorithms, **the relative variation of the objective function should be smaller than $\\epsilon'$** (same as criterion #1 but considering a relative variation) : \n",
    "\n",
    "$$\\frac{\\|f(W_t,H_t) - f(W_{t-1},H_{t-1})\\|^2_F}{\\|f(W_{t-1},H_{t-1})\\|_F^2}  <\\epsilon'$$\n",
    "\n",
    "\n",
    "* the fact that between two iterations of any of the algorithms, **the relative variation of the matrix computed** (for example $H_t$) **should be smaller than $\\epsilon''$** : \n",
    "\n",
    "$$\\frac{\\|H_t - H_{t-1}\\|^2_F}{\\|H_{t-1}\\|_F^2}  <\\epsilon''$$\n",
    "\n",
    "* similar criteria with absolute variations\n",
    "\n",
    "$$\\text{}$$\n",
    "\n",
    "The difficulty with absolute variations (such as criterion number 1) is to find an appropriate threshold. It appears that the relative variations may suit these algorithms better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
